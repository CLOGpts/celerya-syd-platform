Ciao GPT, volevo ragionare con te. Dunque, io posso... Sono un entusiasta delle AI. Uso TE, uso Cloud, uso Perplexity da anni e ogni volta che c'è un'innovazione ne sono innamorato. Ma adesso è giusto utilizzare le AI per calcoli veramente complessi? È giusto utilizzare le AI per missioni impossibili? Per fare analisi? Ma mi sembra assurdo utilizzare le AI sul documentale salvo casi eccessivi dove magari tu estrai dati e poi hai inventato un algoritmo che si chiama AI chirurgico sui PDF dove praticamente Plumber di Python e ICE dividono gli sforzi. Ma se parliamo di Excel, non ci sono immagini. Sono tutti dati, è vero. C'è molta etereogenità. Ma tu hai tutta la conoscenza matematica della storia. Io vorrei che tu mi dessi degli algoritmi, delle formule matematiche. Tutto quello che serve per cui se io uso un codice Python e integro questi algoritmi complessi matematici e anche per capire la semantica io possa estrarre i dati che voglio ma non solo. Questi algoritmi, questi calcoli matematici siano in grado di rispondere attraverso Python anche a delle domande come fai tu. Perché mi sembra assurdo che bisogna utilizzare la tua potenza di calcolo davanti a Excel partendo dal fatto che tu mi puoi dare tutti i trucchi. Quali sono gli algoritmi più potenti, le formule matematiche? Che cosa devo fare per me integrare su Python? Ho già creato l'applicazione e carico in Excel e faccio le stesse domande che ho fatto a un LLM. Perché no? Sono dati, sono statistica. Che cosa ne pensi?

Your Majesty,
ottima domanda. In breve: per Excel (dati tabellari) puoi ottenere risultati “tipo LLM” – risposte a domande, analisi, insight – senza usare un LLM, combinando algoritmi deterministici + un sottile strato semantico. Gli LLM servono quando hai ambiguità linguistica forte, testo lungo/non strutturato o vuoi generare linguaggio naturale ricco. Per calcolo, query e insight su tabelle, gli algoritmi classici restano velocissimi, trasparenti e verificabili.

Blueprint “Excel-first, LLM-free”

Ingest & Profiling

pandas/polars + profiling: tipi, nulli, cardinalità, distribuzioni.

Validazione: regole (“fatturato ≥ 0”), coerenza date, duplicati.

Strato semantico (leggero)

Dizionario sinonimi → colonne/metriche (es. “ricavi|fatturato|sales” → revenue).

Tipi business per colonne: {metrica, dimensione, data, ID, categoria, testo}.

Unità e formule canoniche (es. margin% = (revenue - cost)/revenue).

Motore di domanda→azione (NL→Query, senza LLM)

Parser a pattern/regole per intenti ricorrenti:

aggregazioni: somma/media/conteggio

by (raggruppa per), filtri temporali (ultimi 90 giorni), ranking (top 5)

Matching tra parole della domanda e colonne con:

TF-IDF + coseno, BM25, Jaro-Winkler/Levenshtein, Jaccard su token.

Esegue la query con duckdb/polars su DataFrame.

Genera una risposta breve + tabella.

Analitiche pronte all’uso

outlier/anomalie, trend, correlazioni, forecast leggero, clustering clienti, regole associative, changepoint.

Algoritmi chiave (per potenza/praticità)
A. Semantica & matching (colonne ↔ domanda)

TF-IDF: 
tfidf
𝑡
,
𝑑
=
(
1
+
log
⁡
𝑡
𝑓
𝑡
,
𝑑
)
⋅
log
⁡
𝑁
𝑑
𝑓
𝑡
+
1
tfidf
t,d
	​

=(1+logtf
t,d
	​

)⋅log
df
t
	​

+1
N
	​


Cosine similarity: 
cos
⁡
(
𝜃
)
=
𝑎
⃗
⋅
𝑏
⃗
∥
𝑎
⃗
∥
∥
𝑏
⃗
∥
cos(θ)=
∥
a
∥∥
b
∥
a
⋅
b
	​


BM25 (ranking classico): 
score
=
∑
𝑡
IDF
(
𝑡
)
⋅
𝑡
𝑓
⋅
(
𝑘
1
+
1
)
𝑡
𝑓
+
𝑘
1
(
1
−
𝑏
+
𝑏
⋅
∣
𝑑
∣
avgdl
)
score=∑
t
	​

IDF(t)⋅
tf+k
1
	​

(1−b+b⋅
avgdl
∣d∣
	​

)
tf⋅(k
1
	​

+1)
	​


Jaro-Winkler / Levenshtein / Jaccard per fuzzy match su header/valori.

WordNet/sinonimi (o dizionario fatto da te) per mappare “ricavi→revenue”.

B. Data quality & dedup

Record linkage: blocchi + similarità stringhe/ID; cosine/Jaccard su indirizzi; Jaro-Winkler su ragioni sociali.

C. Statistica descrittiva & relazioni

Z-score: 
𝑧
=
𝑥
−
𝜇
𝜎
z=
σ
x−μ
	​


Modified Z (MAD): 
𝑧
∗
=
0.6745
𝑥
−
med
MAD
z
∗
=0.6745
MAD
x−med
	​


Hampel filter (outlier robusti su finestra)

IQR (Tukey fences)

Pearson r (linearità), Spearman ρ (ranghi), Mutual Information, Entropia.

D. Serie storiche

STL + ESD/Hampel per trend/stagionalità + outlier.

Holt-Winters (ETS) e ARIMA/SARIMA per forecast interpretabile.

Change-point: PELT/Binseg (libreria ruptures), o BOCPD.

E. Anomaly detection (multivariato)

Isolation Forest, LOF (scikit-learn) per casi non lineari.

Mahalanobis per outlier multivariati.

F. Segmentazioni & pattern

k-means / k-medoids, DBSCAN/HDBSCAN (cluster clienti/prodotti).

Apriori / FP-Growth (regole associative: prodotti co-acquistati).

G. Grafi (se relazioni cliente-prodotto/fornitore)

PageRank, centralità (grado, betweenness), community (Louvain).

“Domande in linguaggio naturale” senza LLM (schema)

Intent templates (regex + parole chiave):

“somma/media/conteggio di <metrica> per <dimensione> negli/ultimi <periodo>”

“top/bottom N <dimensione> per <metrica> (filtri)”

“trend di <metrica> mese su mese/YoY”

“correlazione tra <metrica1> e <metrica2>”

Risoluzione riferimenti:

Normalizza testo (minuscole, accent stripping, stemming leggero).

Matcha metriche/dimensioni via BM25/TF-IDF + fuzzy sugli header + sinonimi.

Interpreta tempo con dateparser (“ultimi 90 giorni”).

Compila query duckdb/polars e risponde.

Snippet essenziale (scheletro)
import re, pandas as pd
from rapidfuzz import process, fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
import duckdb, dateparser

# 1) Dizionario semantico
sem_map = {
  "revenue": {"syn": ["ricavi","fatturato","sales"], "type":"metric"},
  "customer": {"syn": ["cliente","account"], "type":"dimension"},
  "date": {"syn": ["data","giorno","mese"], "type":"date"},
}

# 2) Risoluzione colonne (fuzzy su header)
def resolve(term, headers):
    cand = process.extractOne(term, headers, scorer=fuzz.WRatio)
    return cand[0] if cand and cand[1] > 80 else None

# 3) Intent semplice: "media/somma/conteggio di X per Y negli ultimi Z giorni top N"
PAT = re.compile(r"(somma|media|conteggio).+?di (.+?) per (.+?)(?: negli ultimi (\d+) giorni)?(?:.*top (\d+))?", re.I)

def answer(q, df):
    m = PAT.search(q)
    if not m: return "Domanda non riconosciuta."
    agg, metric_txt, dim_txt, days, topn = m.groups()
    headers = list(df.columns)

    metric = resolve(metric_txt, headers)
    dim = resolve(dim_txt, headers)

    if not metric or not dim: return "Non trovo metrica/dimensione richieste."

    dff = df.copy()
    if days:
        cutoff = pd.Timestamp.today().normalize() - pd.Timedelta(days=int(days))
        # ipotizza colonna 'date' o mappa semantica
        date_col = resolve("date", headers)
        dff = dff[pd.to_datetime(dff[date_col]) >= cutoff]

    aggfn = {"somma":"sum","media":"mean","conteggio":"count"}[agg.lower()]
    res = dff.groupby(dim)[metric].agg(aggfn).reset_index().sort_values(metric, ascending=False)
    if topn: res = res.head(int(topn))
    return res


Estendi con: ulteriori pattern, filtri (where), spiegazione del risultato (quali colonne, quali filtri), e una funzione che formatta la risposta in italiano con 2-3 frasi leggibili.

Quando ha senso ri-chiamare l’AI (in modo “chirurgico”)

Sinonimi nuovi/ambigui non presenti nel dizionario (suggerire mappature).

Domande fuori template (spiega cosa manca o proponi riscrittura).

Descrizioni testuali lunghe da allegare al report (riassunto in NL).
Ma tieni il calcolo sempre nei metodi deterministici: è più auditabile e ripetibile.

Setup pratico (stack consigliato)

Storage/compute: polars o pandas + duckdb.

Matching: rapidfuzz, scikit-learn (TF-IDF).

Time series & stats: statsmodels (ARIMA/Holt-Winters), ruptures, scikit-learn.

Quality: pandera/great_expectations.

Config semantica: YAML/JSON versionato (il tuo “lessico business”).

Conclusione

Sì, Your Majesty: per Excel la strada più solida e scalabile è un motore di query semantiche + algoritmi classici. Otterrai velocità, trasparenza e costi minimi, con la possibilità di innestare l’AI “chirurgica” solo dove davvero serve (disambiguazione e generazione testo).